{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9780e6de-5600-44ef-a005-d707c048b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None) ## 레코드의 모든 값을 보기 위한 설정(text안짤리게)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] ='Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] =False\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412742a-3825-452f-ba4e-0990f2897e19",
   "metadata": {},
   "source": [
    "# 4.3 데이터 준비: 레딧 셀프포스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155dce36-5466-4adb-9e8f-05429341686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\user\\Desktop\\DataScience_git\\TextAnalysis\n",
      "['rspct.tsv', 'subreddit_info.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "print(os.listdir('../dataset/RedditSelfPosts/'))\n",
    "# os.chdir('../dataset/RedditSelfPosts/')\n",
    "# print(os.listdir())\n",
    "# os.chdir('../../TextAnalysis/')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0843dc67-b89f-4b9b-a13e-9c77579c2032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1013000 entries, 0 to 1012999\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   id         1013000 non-null  object\n",
      " 1   subreddit  1013000 non-null  object\n",
      " 2   title      1013000 non-null  object\n",
      " 3   selftext   1013000 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 30.9+ MB\n"
     ]
    }
   ],
   "source": [
    "dir='../dataset/RedditSelfPosts/'\n",
    "posts_file = 'rspct.tsv'\n",
    "posts_df=pd.read_csv(dir+posts_file, sep='\\t')\n",
    "posts_df.info() ## 사용자들의 게시글. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeed39d5-27e9-43a2-9e7b-a93e7cc8c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3394 entries, whatsthatbook to Glitch_in_the_Matrix\n",
      "Data columns (total 5 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   category_1            3394 non-null   object\n",
      " 1   category_2            3362 non-null   object\n",
      " 2   category_3            536 non-null    object\n",
      " 3   in_data               3394 non-null   bool  \n",
      " 4   reason_for_exclusion  2381 non-null   object\n",
      "dtypes: bool(1), object(4)\n",
      "memory usage: 135.9+ KB\n"
     ]
    }
   ],
   "source": [
    "subred_file='subreddit_info.csv'\n",
    "subred_df=pd.read_csv(dir+subred_file).set_index(['subreddit'])\n",
    "subred_df.info() ## subreddit 이라는 커뮤니키에 대한 정보들. 프라이머리키는 subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da026510-c46f-4c0c-bda6-d7d13f95f026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1013000 entries, 0 to 1012999\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count    Dtype \n",
      "---  ------                --------------    ----- \n",
      " 0   id                    1013000 non-null  object\n",
      " 1   subreddit             1013000 non-null  object\n",
      " 2   title                 1013000 non-null  object\n",
      " 3   selftext              1013000 non-null  object\n",
      " 4   category_1            1013000 non-null  object\n",
      " 5   category_2            1013000 non-null  object\n",
      " 6   category_3            136000 non-null   object\n",
      " 7   in_data               1013000 non-null  bool  \n",
      " 8   reason_for_exclusion  0 non-null        object\n",
      "dtypes: bool(1), object(8)\n",
      "memory usage: 62.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df=posts_df.join(subred_df, on='subreddit')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e3049-c071-4e9d-9762-063bc384848c",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a9bb2a-4778-47bf-b1d8-f8c9f2151728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>680818</th>\n",
       "      <td>4ry56o</td>\n",
       "      <td>phenibut</td>\n",
       "      <td>UPDATE: I just quit a 15g/day phenibut habit and am wondering if I'll need hospitalization</td>\n",
       "      <td>Thank GOD, I just acquired 4 .5mg Klonopin and took one, and for once can think straight. I still feel quite rocky but I feel like I may want to live for the first time all week. I have only one concern: I want to use one of these .5's a day to get me through these first three horrid days of withdrawal. After about day 4, I can usually manage the withdrawal of phenibut/sub from there, (done it before with less phenibut and waaay more sub). But I must make damn sure that I don't set back my phenibut withdrawal, I'm trying to catapult through these first tough moments. &lt;lb&gt;&lt;lb&gt;Someone pleeeease tell me, truthfully, that this can be done. I have absolutely 0 benzo tolerance and this is honestly the only time I've ever cared for them.&lt;lb&gt;</td>\n",
       "      <td>drugs</td>\n",
       "      <td>phelibut</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436854</th>\n",
       "      <td>81rae0</td>\n",
       "      <td>poetry_critics</td>\n",
       "      <td>Blessed Rest</td>\n",
       "      <td>Did you ever stop and think&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;that every night Christ went to sleep?&lt;lb&gt;&lt;lb&gt;&lt;lb&gt; Did He dream of all His children&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;or creatures in the oceans deep&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Or relive how He made the world&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;and how He first created light,&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;the Master of the universe&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;while sleeping quiet in the night?&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt; &lt;lb&gt;While asleep he sent the Spirit&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Lucid dreams around the world&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Prophecies he gave while sleeping&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;out of mouths of boys and girls&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Did Satan visit in His slumber?&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Did He fight then for our souls,&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;in darkness overcoming Darkness&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;through sleep's relinquished self-control?&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Did He when waking as a child&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;tell Mother nightmares of the cross&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;and how the devil tried to tempt Him&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;in the desert still far off?&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Did He frequent Heaven dreaming&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;who holds the whole world in His hands,&lt;lb&gt;&lt;lb&gt; &lt;lb&gt;and hear there hosts of angels singing&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;to lullaby the Son of Man?&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Did He take counsel with the Father&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;seated on His mighty throne&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;reviewing plans for our salvation&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;on nighttime visits to His home?&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Did He see the Resurrection,&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;His body buried in a tomb&lt;lb&gt;&lt;lb&gt; &lt;lb&gt;our God who saved the human race&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;by ent'ring through a virgin's womb?&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;He did dream then of His Kingdom&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;prepared before the Earth was born&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;and how He'll dwell there with His servants&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;awoken to the final morn&lt;lb&gt;&lt;lb&gt;---&lt;lb&gt;&lt;lb&gt;https://www.reddit.com/r/poetry_critics/comments/81qsx0/would_you/dv4io7o/&lt;lb&gt;&lt;lb&gt;https://www.reddit.com/r/poetry_critics/comments/811n3t/entropy/duztwkw/</td>\n",
       "      <td>writing/stories</td>\n",
       "      <td>poetry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863288</th>\n",
       "      <td>4we4rz</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>My brother (type 2) has an awful sleeping pattern, will this affect his diabetes at all?</td>\n",
       "      <td>Hope this is the right place to post this! So as it says above, my brother (23) goes to bed around 9am and gets up at 5pm, goes to work until midnight, comes home, plays on his pc and then goes to bed (rinse she repeat). My question is, will a lack of vitamin d, not seeing daylight etc affect his diabetes? He's on medication but has been told that he's not managing his condition well enough and might have to go on to insulin, but he's not talking it seriously and hasn't done since he was diagnoses a year ago. &lt;lb&gt;&lt;lb&gt;Thanks in advance!</td>\n",
       "      <td>health</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887978</th>\n",
       "      <td>5nlxww</td>\n",
       "      <td>actuary</td>\n",
       "      <td>IFoA/UK track: predictive modelling and data science</td>\n",
       "      <td>I aware that the CAS is making a serious move to allow actuaries to expand into this area and I was just wondering if this is something which is viable for UK actuaries on the IFoA track, especially given the changes to exam structure which is now going to include R and machine learning.</td>\n",
       "      <td>profession</td>\n",
       "      <td>actuary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716550</th>\n",
       "      <td>8kgcf6</td>\n",
       "      <td>migraine</td>\n",
       "      <td>Does anyone else get straight stupid when having a migraine?</td>\n",
       "      <td>I just came out of a pretty bad one and, among other things, convinced myself that if I shaved my head it would cure my migraines forever. I also straight up forgot I had a tongue and couldn't figure out what was in my mouth. And I always have a hard time picking the right words during and after one. I was wondering if anyone else can relate?</td>\n",
       "      <td>health</td>\n",
       "      <td>migraine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id       subreddit  \\\n",
       "680818  4ry56o        phenibut   \n",
       "436854  81rae0  poetry_critics   \n",
       "863288  4we4rz        diabetes   \n",
       "887978  5nlxww         actuary   \n",
       "716550  8kgcf6        migraine   \n",
       "\n",
       "                                                                                             title  \\\n",
       "680818  UPDATE: I just quit a 15g/day phenibut habit and am wondering if I'll need hospitalization   \n",
       "436854                                                                                Blessed Rest   \n",
       "863288    My brother (type 2) has an awful sleeping pattern, will this affect his diabetes at all?   \n",
       "887978                                        IFoA/UK track: predictive modelling and data science   \n",
       "716550                                Does anyone else get straight stupid when having a migraine?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      selftext  \\\n",
       "680818                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Thank GOD, I just acquired 4 .5mg Klonopin and took one, and for once can think straight. I still feel quite rocky but I feel like I may want to live for the first time all week. I have only one concern: I want to use one of these .5's a day to get me through these first three horrid days of withdrawal. After about day 4, I can usually manage the withdrawal of phenibut/sub from there, (done it before with less phenibut and waaay more sub). But I must make damn sure that I don't set back my phenibut withdrawal, I'm trying to catapult through these first tough moments. <lb><lb>Someone pleeeease tell me, truthfully, that this can be done. I have absolutely 0 benzo tolerance and this is honestly the only time I've ever cared for them.<lb>   \n",
       "436854  Did you ever stop and think<lb><lb><lb>that every night Christ went to sleep?<lb><lb><lb> Did He dream of all His children<lb><lb><lb>or creatures in the oceans deep<lb><lb>&amp;nbsp;<lb><lb><lb><lb>Or relive how He made the world<lb><lb><lb>and how He first created light,<lb><lb><lb>the Master of the universe<lb><lb><lb>while sleeping quiet in the night?<lb><lb>&amp;nbsp;<lb><lb><lb> <lb>While asleep he sent the Spirit<lb><lb><lb>Lucid dreams around the world<lb><lb><lb>Prophecies he gave while sleeping<lb><lb><lb>out of mouths of boys and girls<lb><lb>&amp;nbsp;<lb><lb><lb><lb>Did Satan visit in His slumber?<lb><lb><lb>Did He fight then for our souls,<lb><lb><lb>in darkness overcoming Darkness<lb><lb><lb>through sleep's relinquished self-control?<lb><lb>&amp;nbsp;<lb><lb><lb>Did He when waking as a child<lb><lb><lb>tell Mother nightmares of the cross<lb><lb><lb>and how the devil tried to tempt Him<lb><lb><lb>in the desert still far off?<lb><lb>&amp;nbsp;<lb><lb><lb><lb>Did He frequent Heaven dreaming<lb><lb><lb>who holds the whole world in His hands,<lb><lb> <lb>and hear there hosts of angels singing<lb><lb><lb>to lullaby the Son of Man?<lb><lb>&amp;nbsp;<lb><lb><lb>Did He take counsel with the Father<lb><lb><lb>seated on His mighty throne<lb><lb><lb>reviewing plans for our salvation<lb><lb><lb>on nighttime visits to His home?<lb><lb>&amp;nbsp;<lb><lb><lb>Did He see the Resurrection,<lb><lb><lb>His body buried in a tomb<lb><lb> <lb>our God who saved the human race<lb><lb><lb>by ent'ring through a virgin's womb?<lb><lb>&amp;nbsp;<lb><lb><lb>He did dream then of His Kingdom<lb><lb><lb>prepared before the Earth was born<lb><lb><lb>and how He'll dwell there with His servants<lb><lb><lb>awoken to the final morn<lb><lb>---<lb><lb>https://www.reddit.com/r/poetry_critics/comments/81qsx0/would_you/dv4io7o/<lb><lb>https://www.reddit.com/r/poetry_critics/comments/811n3t/entropy/duztwkw/   \n",
       "863288                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Hope this is the right place to post this! So as it says above, my brother (23) goes to bed around 9am and gets up at 5pm, goes to work until midnight, comes home, plays on his pc and then goes to bed (rinse she repeat). My question is, will a lack of vitamin d, not seeing daylight etc affect his diabetes? He's on medication but has been told that he's not managing his condition well enough and might have to go on to insulin, but he's not talking it seriously and hasn't done since he was diagnoses a year ago. <lb><lb>Thanks in advance!   \n",
       "887978                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        I aware that the CAS is making a serious move to allow actuaries to expand into this area and I was just wondering if this is something which is viable for UK actuaries on the IFoA track, especially given the changes to exam structure which is now going to include R and machine learning.   \n",
       "716550                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                I just came out of a pretty bad one and, among other things, convinced myself that if I shaved my head it would cure my migraines forever. I also straight up forgot I had a tongue and couldn't figure out what was in my mouth. And I always have a hard time picking the right words during and after one. I was wondering if anyone else can relate?   \n",
       "\n",
       "             category_1 category_2 category_3  in_data reason_for_exclusion  \n",
       "680818            drugs   phelibut        NaN     True                  NaN  \n",
       "436854  writing/stories     poetry        NaN     True                  NaN  \n",
       "863288           health   diabetes        NaN     True                  NaN  \n",
       "887978       profession    actuary        NaN     True                  NaN  \n",
       "716550           health   migraine        NaN     True                  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88223a1-6b1d-483e-bc4f-8c7765192488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit        category_1   category_2            category_3\n",
       "ASUS             electronics  laptop/notebook       asus          1000\n",
       "obs              software     video streaming       obs           1000\n",
       "korea            geo          korea                 country       1000\n",
       "lebanon          geo          lebanon               country       1000\n",
       "linuxquestions   software     unix/unix-like        linux         1000\n",
       "                                                                  ... \n",
       "ShieldAndroidTV  electronics  television            shield tv     1000\n",
       "SCCM             software     infrastructure        SCCM          1000\n",
       "Roll20           software     tabletop emulator     roll20        1000\n",
       "Roku             electronics  digital media player  roku          1000\n",
       "ynab             software     budgeting             ynab          1000\n",
       "Name: count, Length: 136, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['subreddit','category_1','category_2','category_3']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897666a2-c98e-435f-9c27-65bd6748a3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['writing/stories', 'tv_show', 'autos', 'hardware/tools',\n",
       "       'electronics', 'video_game', 'crypto', 'sports', 'hobby',\n",
       "       'appearance', 'card_game', 'drugs', 'advice/question',\n",
       "       'social_group', 'anime/manga', 'sex/relationships', 'software',\n",
       "       'health', 'other', 'animals', 'arts', 'programming', 'rpg',\n",
       "       'books', 'parenting', 'education', 'company/website', 'profession',\n",
       "       'music', 'politics/viewpoint', 'stem', 'travel', 'geo',\n",
       "       'religion/supernatural', 'board_game', 'movies', 'food/drink',\n",
       "       'finance/money', 'meta'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category_1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e35a809-ce04-4f7a-8f8b-2020cfdcb666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech support', 'teen mom', 'harley davidson', ..., 'halo',\n",
       "       'grand theft auto', 'mead'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category_2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e43561-9d20-4204-88d4-1a3a80b9d939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'intel', 'oculus rift', 'garmin', 'sharepoint', 'gamestop',\n",
       "       'uber', 'cloud9', 'microsoft', 'country', 'home assistant', 'tor',\n",
       "       'hue', 'costco', 'taco bell', 'movie pass club', 'snapchat',\n",
       "       'excel', 'synology', 'dji', 'wacom', 'sony', 'obs', 'nest',\n",
       "       'vmware', 'knife club', 'starbucks', 'android auto', 'tinder',\n",
       "       'valve', 'magento', 'roku', 'autohotkey', 'solidworks', 'thinkpad',\n",
       "       'premiere', 'ynab', 'corsair', 'spacex', 'unreal', 'windows',\n",
       "       'lineage os', 'cemu', 'amazon', 'adobe illustrator', 'discord',\n",
       "       'mac os', 'pfsense', 'postmates', 'outlook', 'arduino', 'sonarr',\n",
       "       'verizon', 'instacart', 'google', 'criterion', 'shield tv',\n",
       "       'roll20', 'nvidia', 'tableau', 'samsung', 'word press', 'origin',\n",
       "       'boosted', 'tasker', 'onenote', 'lime technology', 'cisco',\n",
       "       'crossfit', 'if then then that', 'nintendo', 'kodi', 'homelab',\n",
       "       'audible', 'u block', 'walmart', 'airbnb', 'home depot', 'swift',\n",
       "       'kickstarter', 'workflow', 'SCCM', 'staples', 'pihole',\n",
       "       'salesforce', 'linux', 'beauty box', 'firefox', 'juul', 'asus',\n",
       "       'radarr', 'gopro', 'fitbit', 'raspberrypi', 'rainmeter', 'anki',\n",
       "       'microsoft access', 'ableton', 'ikea', 'freenas', 'paypal',\n",
       "       'apple', 'spotify', 'blender', 'pebble', 'trezor'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category_3'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07a0882e-8639-4b80-81e3-7bd0731deb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            0\n",
       "subreddit                     0\n",
       "title                         0\n",
       "selftext                      0\n",
       "category_1                    0\n",
       "category_2                    0\n",
       "category_3               877000\n",
       "in_data                       0\n",
       "reason_for_exclusion    1013000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea38d87-9a11-4a65-ac84-b0150d7cfaeb",
   "metadata": {},
   "source": [
    "## 컬럼(속성) 표준화\n",
    "- 1. 사용하지 않을 컬럼과 사용할 컬럼으로 나눈다.\n",
    "  2. 사용할 컬럼의 이름을 일반적인 컬럼명으로 변경해주고, 사용하지 않을 컬럼의 이름에는 None을 붙인다.\n",
    "  3. 사용하지 않을 컬럼을 삭제한다.\n",
    "- 목적: 100만건의 데이터는 너무 방대하므로, 카테고리가 \"autos\"인 데이터 2만건만 추출해낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2667346-5f4f-49cf-bb92-0380ccf906a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'subreddit', 'title', 'selftext', 'category_1', 'category_2',\n",
       "       'category_3', 'in_data', 'reason_for_exclusion'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17e05d71-c0ac-4b23-938d-9b20662f2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['id', 'subreddit', 'title', 'text', 'category', 'subcategory',\n",
    "            None, None, None] \n",
    "# 필요 없는 카테고리3, 인데이터, 배제사유 날려버리기\n",
    "df=df.drop([col for col in df.columns if col==None],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1603162f-ec30-4736-898f-23ec1182531c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1004493</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>8ajbif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <td>SCP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>I have a several questions about incident 096-1-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>In [Incident 096-1-A](http://www.scp-wiki.net/incident-096-1-a), which is the account of the containment breach and termination of SCP-096 for those haven’t read it (which I suggest you do, it’s a great read), there were a couple things I didn’t understand or just might have missed.&lt;lb&gt;&lt;lb&gt;1. How were they able to terminate SCP-096? In the initial retrieval interview, they mention blowing out half of its torso with a anti tank gun. My guess is that it was killable when not in rage state, but I didn’t see that information anywhere in the SCP and would like someone to confirm that. Also, after initial retrieval, did it just have a hole in its side for the entirety of being at whatever Site it was at, or can it regenerate?&lt;lb&gt;&lt;lb&gt;2. How did mountain guy not trigger SCP-096 beforehand? It’s mentioned that you don’t need to consciously recognize the face, just see it to trigger 096, but then how is it possible for the mountaineer to have not triggered him for that long? Did he somehow never look at his photo?&lt;lb&gt;&lt;lb&gt;3. My biggest question: Why were Dr. Dan and Dr. Oleksei terminated? For Dan, I assume it was because the SCRAMBLE gear failed, but that doesn’t seem like a reason to kill him after terminating 096. Was his sole job 096 and after it was terminated they didn’t need him around anymore and he knew too much? It just seems like a waste of higher class personnel. Dr. Oleksei on the other hand, makes even less sense, or more particularly, is just vague. Early in the incident report, Oleksei mentions not being killed by 096 because he was in the break room getting coffee when 096 breached containment, but at the end it is revealed that Research Site ██ doesn’t have a break room, and that there “was no use to play dumb” and that “he told us everything”. Who told them everything? Did I miss something important? The only thing I can think of is Dr. Dan and Dr. Oleksei purposely breaching containment to test the SCRAMBLE gear, but I don’t see how that’s possible considering the breach was due to a photo a guy had for an undisclosed amount of years.&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Sorry for the long post, but currently SCP-096 and all of the incident reports are my one of my favorite ones on the wiki and I just wanted to fully understand what happened. If anyone could clear some of this up, I would be super grateful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subcategory</th>\n",
       "      <td>scp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1004493\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                8ajbif\n",
       "subreddit                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            SCP\n",
       "title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  I have a several questions about incident 096-1-A\n",
       "text         In [Incident 096-1-A](http://www.scp-wiki.net/incident-096-1-a), which is the account of the containment breach and termination of SCP-096 for those haven’t read it (which I suggest you do, it’s a great read), there were a couple things I didn’t understand or just might have missed.<lb><lb>1. How were they able to terminate SCP-096? In the initial retrieval interview, they mention blowing out half of its torso with a anti tank gun. My guess is that it was killable when not in rage state, but I didn’t see that information anywhere in the SCP and would like someone to confirm that. Also, after initial retrieval, did it just have a hole in its side for the entirety of being at whatever Site it was at, or can it regenerate?<lb><lb>2. How did mountain guy not trigger SCP-096 beforehand? It’s mentioned that you don’t need to consciously recognize the face, just see it to trigger 096, but then how is it possible for the mountaineer to have not triggered him for that long? Did he somehow never look at his photo?<lb><lb>3. My biggest question: Why were Dr. Dan and Dr. Oleksei terminated? For Dan, I assume it was because the SCRAMBLE gear failed, but that doesn’t seem like a reason to kill him after terminating 096. Was his sole job 096 and after it was terminated they didn’t need him around anymore and he knew too much? It just seems like a waste of higher class personnel. Dr. Oleksei on the other hand, makes even less sense, or more particularly, is just vague. Early in the incident report, Oleksei mentions not being killed by 096 because he was in the break room getting coffee when 096 breached containment, but at the end it is revealed that Research Site ██ doesn’t have a break room, and that there “was no use to play dumb” and that “he told us everything”. Who told them everything? Did I miss something important? The only thing I can think of is Dr. Dan and Dr. Oleksei purposely breaching containment to test the SCRAMBLE gear, but I don’t see how that’s possible considering the breach was due to a photo a guy had for an undisclosed amount of years.<lb><lb><lb>Sorry for the long post, but currently SCP-096 and all of the incident reports are my one of my favorite ones on the wiki and I just wanted to fully understand what happened. If anyone could clear some of this up, I would be super grateful.\n",
       "category                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           other\n",
       "subcategory                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          scp"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.sample(1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad00d366-7902-4395-bf6b-aada2ec53543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20000 entries, 2 to 1012979\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           20000 non-null  object\n",
      " 1   subreddit    20000 non-null  object\n",
      " 2   title        20000 non-null  object\n",
      " 3   text         20000 non-null  object\n",
      " 4   category     20000 non-null  object\n",
      " 5   subcategory  20000 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "## \"autos\" category 추출\n",
    "df = df[df['category']=='autos']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd95e134-b5d9-4ddb-a810-e1bb5cd72cd4",
   "metadata": {},
   "source": [
    "## sqlite3 : autos 데이터프레임 SQL DB로 저장하기\n",
    "- 피클은 파이썬에서만 사용할 수 있다.\n",
    "- 데이터프레임은 SQL의 이점을 이용할 수 있으므로, SQL Lite으로 저장한다.\n",
    "\n",
    "- - 아래는 df.to_sql 함수의 주요 파라미터들입니다. \n",
    "- 1. name: DataFrame을 쓸 SQL 테이블의 이름입니다.\n",
    "  2. con: SQLAlchemy 연결 객체입니다. 데이터베이스 연결 정보를 설정합니다.\n",
    "  3. schema: 테이블의 스키마 이름 (기본값은 None).\n",
    "  4. if_exists: 테이블이 이미 존재할 경우의 동작을 지정합니다. 옵션으로는 'fail' (기본값, 테이블이 이미 존재하면 실패), 'replace' (테이블을 덮어씀), 'append' (테이블에 추가)이 있습니다.\n",
    "  5. index: DataFrame의 인덱스를 데이터베이스 테이블의 인덱스로 사용할지 여부를 지정합니다 (기본값은 True).\n",
    "  6. index_label: 인덱스 열의 이름을 지정합니다.\n",
    "  7. chunksize: 한 번에 쓰는 레코드 수를 지정합니다.\n",
    "  8. dtype: 열의 데이터 유형을 지정합니다.\n",
    "  9. method: 데이터를 쓰는 방법을 지정합니다.방법을 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30b22b08-4244-44e3-b5d8-3d1aa7ef827c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 저장\n",
    "import sqlite3\n",
    "save_dir=\"../output/\"\n",
    "db_name=\"reddit-selfposts.db\"\n",
    "con=sqlite3.connect(save_dir+db_name)\n",
    "df.to_sql(\"posts\", con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e075e0d6-8929-4652-8da4-6463e44d8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 읽기\n",
    "save_dir=\"../output/\"\n",
    "db_name=\"reddit-selfposts.db\"\n",
    "con=sqlite3.connect(save_dir+db_name)\n",
    "df=pd.read_sql(\"select * from posts\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fcbb505-c72e-461c-857e-cb99f6426804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We always ended up at a bar called Hogs &amp;amp; Heifers. It's worth noting the females working there can outdrink ANYONE. Anyway, there was a sign on the front door that read 'No Club Colors'. So we lose our ties and blazers before heading there. Also we assumed bright colors like red, yellow, green etc were not allowed. So we would always bring an xtra t-shirt and pair of jeans. This went on for years! Looking back now on how naive we were, it's just hilarious. I was never able to walk out of that bar....had to crawl out! So much booze. &lt;lb&gt;&lt;lb&gt;Cheers. Ride safe, boys!</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5s0q8r</td>\n",
       "      <td>Mustang</td>\n",
       "      <td>Roush vs Shleby GT500</td>\n",
       "      <td>I am trying to determine which is faster, and I've seen the dealership video with the two racing(Roush won 2/3). But I was wondering if it was just because of the bigass supercharger in the Roush. &lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Also I can't find the same specs on any two websites, what are some trustworthy sources for this kind of thing?</td>\n",
       "      <td>autos</td>\n",
       "      <td>ford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5z3405</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>2001 Golf Wagon looking for some insight</td>\n",
       "      <td>Hello! &lt;lb&gt;&lt;lb&gt;Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... mine's gone out and going to the dealership is quite out of the range right now. Nor is it in the biggest of rushes. &lt;lb&gt;&lt;lb&gt;I live in Japan, and I'm wondering if anyone had any international shipping websites they could recommend and if they had any direction on how to replace it as manuals are oh-so-specific (and not to mention, in Japanese). TIA</td>\n",
       "      <td>autos</td>\n",
       "      <td>VW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7df18v</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>IS 250 Coolant Flush/Change</td>\n",
       "      <td>https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/&lt;lb&gt;&lt;lb&gt;I have a IS 250 AWD from 2006. About 73K miles on it. I've never touched the engine radiator coolant and can't find anything on when to change this in the book. It just says 'long life 100k Toyota coolant.' &lt;lb&gt;&lt;lb&gt;Does anyone get this flushed or changed at ten years?? Do I wait until 100k?</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5tpve8</td>\n",
       "      <td>volt</td>\n",
       "      <td>Gen1 mpg w/ dead battery?</td>\n",
       "      <td>Hi, new to this subreddit.  I'm considering buying a Gen1 Volt, but I can't find any straight answers as to what kind of mpg it gets after the battery is completely dead (say I take a 300 mile trip).  What kind of highway mpg does the Gen1 volt get after the battery is depleted?</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>7i2k6y</td>\n",
       "      <td>4Runner</td>\n",
       "      <td>Bilstein Shocks</td>\n",
       "      <td>I read a lot Forums and people recommend getting TUNDRA Bilstein Shocks for a 3rd gen 4 runner, what is the difference? and why do they recommend that? I bought Springs tundra Springs for the front and 1997 landcruiser springs fro the rear now I just need shocks. &lt;lb&gt;Thank you</td>\n",
       "      <td>autos</td>\n",
       "      <td>toyota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>83p2kv</td>\n",
       "      <td>Harley</td>\n",
       "      <td>Question on potential purchase of crashed bike.</td>\n",
       "      <td>I am thinking about  buying a 2010 Harley Sportster 1200 custom for $6k with 7k miles. It has Vance radius pipes, t bars, screaming eagle filters, sissy bar, new seat and either a new tank or a new paint job as it does not say Harley Davidson on the tank anywhere but has pin stripes.. the title it’s clean but it just seems to good to be true and the sales man has no record of it being in a accident or not. The Title has been transferred to 5 different owners (can not see prices) I really want this bike but I’m skeptical. Any ideas or tips to look for?</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>7x722h</td>\n",
       "      <td>volt</td>\n",
       "      <td>Got our first warning light on our dash</td>\n",
       "      <td>My husband and I were headed somewhere and I was cold. So, I figured I could turn the car on before he unplugged it. When it turned on it was fine. Then he got into the drivers seat ten seconds later and we got a \"check charging system\" any suggestions? I figure I messed it up by turning it on before unplugging it *facepalm*</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>7v2xmg</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>Any IS models to avoid?</td>\n",
       "      <td>I am looking at getting a used Lexus IS (2014 model year and newer). Are there any trim levels that I should avoid? Thinking about getting a 250, however, I might spend a little bit more and get a 350 if there is a significant performance increase, but I wanted to know if there is anything I should know about the lineup before I pull the trigger. I am really liking the white exterior and red interior combo.</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>8dxx3b</td>\n",
       "      <td>mercedes_benz</td>\n",
       "      <td>Advice please. Looking at a 2011 E550 with 71K miles. Good car?</td>\n",
       "      <td>Looking for some help. I've never owned any luxury car. Especially not a Mercedes. My husband's old commander in the Air Force is leaving the area and offering us his E550 for $15K. &lt;lb&gt;&lt;lb&gt;Seems like a good price but I have no idea about a German car as all I've ever owned are cars less than $25K new. It's loaded with every feature but NOT a 4Matic or AMG. We're test driving it tomorrow. Should we get it inspected? Anyone else own this or a similar model? Thanks!&lt;lb&gt;&lt;lb&gt;I drive for Uber p/t so might be using it for that too.</td>\n",
       "      <td>autos</td>\n",
       "      <td>mercedes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      subreddit  \\\n",
       "0      8f73s7         Harley   \n",
       "1      5s0q8r        Mustang   \n",
       "2      5z3405     Volkswagen   \n",
       "3      7df18v          Lexus   \n",
       "4      5tpve8           volt   \n",
       "...       ...            ...   \n",
       "19995  7i2k6y        4Runner   \n",
       "19996  83p2kv         Harley   \n",
       "19997  7x722h           volt   \n",
       "19998  7v2xmg          Lexus   \n",
       "19999  8dxx3b  mercedes_benz   \n",
       "\n",
       "                                                                 title  \\\n",
       "0                                                       No Club Colors   \n",
       "1                                                Roush vs Shleby GT500   \n",
       "2                             2001 Golf Wagon looking for some insight   \n",
       "3                                          IS 250 Coolant Flush/Change   \n",
       "4                                            Gen1 mpg w/ dead battery?   \n",
       "...                                                                ...   \n",
       "19995                                                  Bilstein Shocks   \n",
       "19996                  Question on potential purchase of crashed bike.   \n",
       "19997                          Got our first warning light on our dash   \n",
       "19998                                          Any IS models to avoid?   \n",
       "19999  Advice please. Looking at a 2011 E550 with 71K miles. Good car?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \\\n",
       "0      Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We always ended up at a bar called Hogs &amp; Heifers. It's worth noting the females working there can outdrink ANYONE. Anyway, there was a sign on the front door that read 'No Club Colors'. So we lose our ties and blazers before heading there. Also we assumed bright colors like red, yellow, green etc were not allowed. So we would always bring an xtra t-shirt and pair of jeans. This went on for years! Looking back now on how naive we were, it's just hilarious. I was never able to walk out of that bar....had to crawl out! So much booze. <lb><lb>Cheers. Ride safe, boys!    \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                  I am trying to determine which is faster, and I've seen the dealership video with the two racing(Roush won 2/3). But I was wondering if it was just because of the bigass supercharger in the Roush. <lb><lb><lb>Also I can't find the same specs on any two websites, what are some trustworthy sources for this kind of thing?   \n",
       "2                                                                                                                                                                                                                                                                                                                                    Hello! <lb><lb>Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... mine's gone out and going to the dealership is quite out of the range right now. Nor is it in the biggest of rushes. <lb><lb>I live in Japan, and I'm wondering if anyone had any international shipping websites they could recommend and if they had any direction on how to replace it as manuals are oh-so-specific (and not to mention, in Japanese). TIA   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                 https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/<lb><lb>I have a IS 250 AWD from 2006. About 73K miles on it. I've never touched the engine radiator coolant and can't find anything on when to change this in the book. It just says 'long life 100k Toyota coolant.' <lb><lb>Does anyone get this flushed or changed at ten years?? Do I wait until 100k?    \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Hi, new to this subreddit.  I'm considering buying a Gen1 Volt, but I can't find any straight answers as to what kind of mpg it gets after the battery is completely dead (say I take a 300 mile trip).  What kind of highway mpg does the Gen1 volt get after the battery is depleted?    \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ...   \n",
       "19995                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         I read a lot Forums and people recommend getting TUNDRA Bilstein Shocks for a 3rd gen 4 runner, what is the difference? and why do they recommend that? I bought Springs tundra Springs for the front and 1997 landcruiser springs fro the rear now I just need shocks. <lb>Thank you   \n",
       "19996                                                                                                                                                                                                                 I am thinking about  buying a 2010 Harley Sportster 1200 custom for $6k with 7k miles. It has Vance radius pipes, t bars, screaming eagle filters, sissy bar, new seat and either a new tank or a new paint job as it does not say Harley Davidson on the tank anywhere but has pin stripes.. the title it’s clean but it just seems to good to be true and the sales man has no record of it being in a accident or not. The Title has been transferred to 5 different owners (can not see prices) I really want this bike but I’m skeptical. Any ideas or tips to look for?   \n",
       "19997                                                                                                                                                                                                                                                                                                                                                                                                                                                        My husband and I were headed somewhere and I was cold. So, I figured I could turn the car on before he unplugged it. When it turned on it was fine. Then he got into the drivers seat ten seconds later and we got a \"check charging system\" any suggestions? I figure I messed it up by turning it on before unplugging it *facepalm*   \n",
       "19998                                                                                                                                                                                                                                                                                                                                                                    I am looking at getting a used Lexus IS (2014 model year and newer). Are there any trim levels that I should avoid? Thinking about getting a 250, however, I might spend a little bit more and get a 350 if there is a significant performance increase, but I wanted to know if there is anything I should know about the lineup before I pull the trigger. I am really liking the white exterior and red interior combo.   \n",
       "19999                                                                                                                                                                                                                                           Looking for some help. I've never owned any luxury car. Especially not a Mercedes. My husband's old commander in the Air Force is leaving the area and offering us his E550 for $15K. <lb><lb>Seems like a good price but I have no idea about a German car as all I've ever owned are cars less than $25K new. It's loaded with every feature but NOT a 4Matic or AMG. We're test driving it tomorrow. Should we get it inspected? Anyone else own this or a similar model? Thanks!<lb><lb>I drive for Uber p/t so might be using it for that too.   \n",
       "\n",
       "      category      subcategory  \n",
       "0        autos  harley davidson  \n",
       "1        autos             ford  \n",
       "2        autos               VW  \n",
       "3        autos            lexus  \n",
       "4        autos        chevrolet  \n",
       "...        ...              ...  \n",
       "19995    autos           toyota  \n",
       "19996    autos  harley davidson  \n",
       "19997    autos        chevrolet  \n",
       "19998    autos            lexus  \n",
       "19999    autos         mercedes  \n",
       "\n",
       "[20000 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe959fc0-aea1-4526-8390-e723189b03d6",
   "metadata": {},
   "source": [
    "# 4.4 텍스트 데이터 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e65b4-0814-422d-85fd-ad5d18994699",
   "metadata": {},
   "source": [
    "## 정규표현식: 노이즈 식별 - impurity 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f97bb90-18ad-415a-8b0d-8428d3ca5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "I am looking at getting a used Lexus IS (2014 model year and newer). \n",
    "Are there any trim levels that I should avoid? \n",
    "Thinking about getting a 250, however, I might spend a little bit more and get a 350 if there is a significant performance increase, \n",
    "but I wanted to know if there is anything I should know about the lineup before I pull the trigger. \n",
    "I am really liking the white exterior and red interior combo.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb5fd258-bb6f-42a7-912c-c82f4c9bc7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "RE_SUSPICIOUS =re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len=10):\n",
    "    \"\"\"텍스트에서 의심스러운 문자의 비율을 반환한다.\"\"\"\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else: \n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
    "\n",
    "print(impurity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f69f3ad-99b4-4aa9-a0cd-618f0f4c4f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08968609865470852\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"\n",
    "After viewing the [PINKIEPOOL Trailer](https://www.youtube.com/watch?v=ieRouHUg) it got me thinking about the best match ups. <lb> Here's my take: <lb><lb>[](/ppseesyou) Deadpool<lb>[](\\sp)[](/ajsly) Captain America <\\lb>\n",
    "\"\"\"\n",
    "print(impurity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89efe735-e9a2-4e0e-b117-a3f6b0088ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19682</th>\n",
       "      <td>Looking at buying a 335i with 39k miles and 11 months left on the CPO warranty. I asked the deal...</td>\n",
       "      <td>0.214716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12357</th>\n",
       "      <td>I'm looking to lease an a4 premium plus automatic with the nav package.&lt;lb&gt;&lt;lb&gt;Vehicle Price:&lt;ta...</td>\n",
       "      <td>0.165099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>Breakdown below:&lt;lb&gt;&lt;lb&gt;Elantra GT&lt;lb&gt;&lt;lb&gt;2.0L 4-cylinder&lt;lb&gt;&lt;lb&gt;6-speed Manual Transmission&lt;lb&gt;...</td>\n",
       "      <td>0.139130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      text  \\\n",
       "19682  Looking at buying a 335i with 39k miles and 11 months left on the CPO warranty. I asked the deal...   \n",
       "12357  I'm looking to lease an a4 premium plus automatic with the nav package.<lb><lb>Vehicle Price:<ta...   \n",
       "2730   Breakdown below:<lb><lb>Elantra GT<lb><lb>2.0L 4-cylinder<lb><lb>6-speed Manual Transmission<lb>...   \n",
       "\n",
       "       impurity  \n",
       "19682  0.214716  \n",
       "12357  0.165099  \n",
       "2730   0.139130  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임에 새로운 컬럼 추가\n",
    "df['impurity']= df['text'].apply(impurity, min_len=10)\n",
    "\n",
    "# 상위 3개 레코드 확인\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df[['text','impurity']].sort_values(by='impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6a6e9eb-3dc2-4cd4-8073-1be431e9eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from blueprints.exploration import count_words\n",
    "# count_words(df, column='text', preprocess= lambda t: re.findall(r'<[\\w/]*>',t))\n",
    "## 블루프린트 없음..\n",
    "\n",
    "# from blueprints.exploration import count_words\n",
    "# count_words(df, column='text', preprocess=lambda t: re.findall(r'<[\\w/]*>', t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d87e434-a0dd-4515-9364-efc9c745ac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<tab>', '<lb>'}\n"
     ]
    }
   ],
   "source": [
    "impurity_set=set()\n",
    "for l in df['text'].apply(lambda t: re.findall(r'<[\\w/]*>',t)).values:\n",
    "    impurity_set|=set(l)\n",
    "print(impurity_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f46bb2-ae80-4871-b60a-60f4b8aa269e",
   "metadata": {},
   "source": [
    "## 정규표현식: 노이즈 제거 - clean함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89a6348b-3187-4818-8a83-0d13bd6b48b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After viewing the PINKIEPOOL Trailer it got me thinking about the best match ups. Here's my take: Deadpool Captain America\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "def clean(text):\n",
    "    # &amp;와 같은 html 이스케이프를 문자로 변환한다.\n",
    "    text=html.unescape(text)\n",
    "    # print(text)\n",
    "    # <tab>, <lb>과 같은 태그를 공백으로 변환한다.\n",
    "    text=re.sub('<[^<>&]*>', ' ', text)\n",
    "    # print(text)\n",
    "    # [Some text](https://....)과 같은 마크다운 URL을 공백으로 변환한다.\n",
    "    # text=re.sub(r'\\[([^\\[\\]]*)\\]([^\\(\\)]*\\)', r'\\1', text)\n",
    "    text = re.sub('\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # print(text)\n",
    "    # [0]과 같은 괄호안의 텍스트 또는 코드를 공백으로 변환한다.\n",
    "    text=re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # print(text)\n",
    "    # 특수 문자로만 구성된 문자열을 공백으로 변환한다. 이때 &#은 변환되지만 #cool은 변환되지 않는다.\n",
    "    text=re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # print(text)\n",
    "    # --- 또는 == 같은 하이픈으로 이뤄진 문자열을 공백으로 변환한다.\n",
    "    text=re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # print(text)\n",
    "    # 연속된 공백을 공백 하나로 변환한다.\n",
    "    text=re.sub('\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "    \n",
    "clean_text=clean(text)\n",
    "print(clean_text)\n",
    "print(impurity(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37f2038b-80aa-4cdb-b4a8-4250c5f21594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>Mustang 2018, 2019, or 2020? Must Haves!! 1. Have a Credit score of 780\\+ for the best low inter...</td>\n",
       "      <td>0.030864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18934</th>\n",
       "      <td>At the dealership, they offered an option for foot-well illumination, but I cannot find any refe...</td>\n",
       "      <td>0.026455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16505</th>\n",
       "      <td>I am looking at four Caymans, all are in a similar price range. The major differences are the mi...</td>\n",
       "      <td>0.024631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                clean_text  \\\n",
       "14058  Mustang 2018, 2019, or 2020? Must Haves!! 1. Have a Credit score of 780\\+ for the best low inter...   \n",
       "18934  At the dealership, they offered an option for foot-well illumination, but I cannot find any refe...   \n",
       "16505  I am looking at four Caymans, all are in a similar price range. The major differences are the mi...   \n",
       "\n",
       "       impurity  \n",
       "14058  0.030864  \n",
       "18934  0.026455  \n",
       "16505  0.024631  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text']=df['text'].map(clean)\n",
    "df['impurity']=df['clean_text'].map(impurity)\n",
    "df[['clean_text','impurity']].sort_values(by='impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed8f8c-6ae1-4092-87d9-fb011e24ac5c",
   "metadata": {},
   "source": [
    "## textacy: 문자 정규화 - normalize, replace, remove\n",
    "- 텍스트 전처리(정규화)에 특화된 라이브러리\n",
    "- 함수\n",
    "- - 1) normalize.hyphenated_words: 줄바꿈으로 구분된 단어를 다시 조합한다.\n",
    "    2) normalize.quotation_marks: 모든 종류의 따옴표를 아스키 코드에 해당하는 곧은 따옴표로 대체한다.\n",
    "    3) normalize.unicode: 유니코드에서 악센트가 있는 문자를 다른 코드로 변경한다.\n",
    "    4) normalize.accents: 가능한 경우 악센트가 있는 문자를 ASCII로 바꾸거나 삭제한다.\n",
    "    5) replace.urls: https://xyz.com과 같은 URL을 _URL_로 대체한다.\n",
    "    6) replace.emails: 이메일을 _EMAILS_로 대체한다.\n",
    "    7) replace.hashtags: #sunshine과 같은 해시태그를 _TAG_로 대체한다.\n",
    "    8) replace.numbers: 12345와 같은 숫자 패턴을 _NUMBER_로 대체한다.\n",
    "    9) replace.phon_numbers: 전화번호 패턴을 _PHONE_으로 대체한다.\n",
    "    10) replace.user_handles: @peter와 같은 사용자명을 _USER_로 대체한다.\n",
    "    11) replace.emojis: 스마일리와 같은 이모지를 _EMOJI_로 대체한다.\n",
    "- https://textacy.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dadd5fb-6e06-47cc-9e85-982364370d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.preprocessing as tprep\n",
    "\n",
    "def normalize(text):\n",
    "    text=tprep.normalize.hyphenated_words(text)\n",
    "    text=tprep.normalize.quotation_marks(text)\n",
    "    text=tprep.normalize.unicode(text)\n",
    "    text=tprep.remove.accents(text)\n",
    "\n",
    "    return(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30a6e6c8-f550-4662-b827-b72daacb525a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cafe \"Saint-Raphael\" is located on Cote d'Azur.\n"
     ]
    }
   ],
   "source": [
    "text=\"The café “Saint-Raphaël” is loca-\\nted on Côte dʼAzur.\"\n",
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cb36b-6a9d-465f-94c7-ca4b2956ccb1",
   "metadata": {},
   "source": [
    "## textacy: 데이터 마스킹 - resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50087d45-bec4-4989-98e4-1f116986c6ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.nrdc.org/experts/luke-tonachel/new-york-state-enacts-electric-vehicle-consumer-rebate-program', 'https://www.nysenate.gov/legislation', 'http://www.vectoroffroad.com/jke-dock-2007-2010-p-28.html!,', 'http://www.morris4x4center.com/rugged-ridge-cb-radio-dash-mounting-bracket-13551-09.html!', 'https://www.thingiverse.com/thing:912478']\n",
      "1152\n"
     ]
    }
   ],
   "source": [
    "from textacy.preprocessing.resources import RE_URL\n",
    "\n",
    "# df['clean_text'].apply(lambda x: RE_URL.findall(x))\n",
    "urls=df['clean_text'].map(RE_URL.findall).values\n",
    "urls_list=[]\n",
    "for u in urls.tolist():\n",
    "    if len(u)>1:\n",
    "        urls_list.extend(u)\n",
    "print(urls_list[:5])\n",
    "print(len(urls_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f9e3060-079f-46b1-b7fb-56fd08284449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check this url change http://i.imgur.com/lcr0MZW.jpg\n",
      "check this url change _URL_\n"
     ]
    }
   ],
   "source": [
    "text=\"check this url change \"+urls_list[101]\n",
    "print(text)\n",
    "\n",
    "print(tprep.replace.urls(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b743b18-022a-4bc4-a743-da91d6935fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'text':'raw_text', 'clean_text':'text'}, inplace=True)\n",
    "df.drop(columns=['impurity'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9665f6c5-33a8-4474-9eea-8f468bfb98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "con=sqlite3.connect(save_dir+db_name)\n",
    "df.to_sql(\"posts_cleaned\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17428ce9-c18e-43ed-8e7c-681d61fd7236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5s0q8r</td>\n",
       "      <td>Mustang</td>\n",
       "      <td>Roush vs Shleby GT500</td>\n",
       "      <td>I am trying to determine which is faster, and I've seen the dealership video with the two racing...</td>\n",
       "      <td>autos</td>\n",
       "      <td>ford</td>\n",
       "      <td>I am trying to determine which is faster, and I've seen the dealership video with the two racing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5z3405</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>2001 Golf Wagon looking for some insight</td>\n",
       "      <td>Hello! &lt;lb&gt;&lt;lb&gt;Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>VW</td>\n",
       "      <td>Hello! Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... mine's g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7df18v</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>IS 250 Coolant Flush/Change</td>\n",
       "      <td>https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/&lt;lb&gt;&lt;lb&gt;I h...</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "      <td>https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/ I have a I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5tpve8</td>\n",
       "      <td>volt</td>\n",
       "      <td>Gen1 mpg w/ dead battery?</td>\n",
       "      <td>Hi, new to this subreddit.  I'm considering buying a Gen1 Volt, but I can't find any straight an...</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>Hi, new to this subreddit. I'm considering buying a Gen1 Volt, but I can't find any straight ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>7i2k6y</td>\n",
       "      <td>4Runner</td>\n",
       "      <td>Bilstein Shocks</td>\n",
       "      <td>I read a lot Forums and people recommend getting TUNDRA Bilstein Shocks for a 3rd gen 4 runner, ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>toyota</td>\n",
       "      <td>I read a lot Forums and people recommend getting TUNDRA Bilstein Shocks for a 3rd gen 4 runner, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>83p2kv</td>\n",
       "      <td>Harley</td>\n",
       "      <td>Question on potential purchase of crashed bike.</td>\n",
       "      <td>I am thinking about  buying a 2010 Harley Sportster 1200 custom for $6k with 7k miles. It has Va...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "      <td>I am thinking about buying a 2010 Harley Sportster 1200 custom for $6k with 7k miles. It has Van...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>7x722h</td>\n",
       "      <td>volt</td>\n",
       "      <td>Got our first warning light on our dash</td>\n",
       "      <td>My husband and I were headed somewhere and I was cold. So, I figured I could turn the car on bef...</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>My husband and I were headed somewhere and I was cold. So, I figured I could turn the car on bef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>7v2xmg</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>Any IS models to avoid?</td>\n",
       "      <td>I am looking at getting a used Lexus IS (2014 model year and newer). Are there any trim levels t...</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "      <td>I am looking at getting a used Lexus IS (2014 model year and newer). Are there any trim levels t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>8dxx3b</td>\n",
       "      <td>mercedes_benz</td>\n",
       "      <td>Advice please. Looking at a 2011 E550 with 71K miles. Good car?</td>\n",
       "      <td>Looking for some help. I've never owned any luxury car. Especially not a Mercedes. My husband's ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>mercedes</td>\n",
       "      <td>Looking for some help. I've never owned any luxury car. Especially not a Mercedes. My husband's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      subreddit  \\\n",
       "0      8f73s7         Harley   \n",
       "1      5s0q8r        Mustang   \n",
       "2      5z3405     Volkswagen   \n",
       "3      7df18v          Lexus   \n",
       "4      5tpve8           volt   \n",
       "...       ...            ...   \n",
       "19995  7i2k6y        4Runner   \n",
       "19996  83p2kv         Harley   \n",
       "19997  7x722h           volt   \n",
       "19998  7v2xmg          Lexus   \n",
       "19999  8dxx3b  mercedes_benz   \n",
       "\n",
       "                                                                 title  \\\n",
       "0                                                       No Club Colors   \n",
       "1                                                Roush vs Shleby GT500   \n",
       "2                             2001 Golf Wagon looking for some insight   \n",
       "3                                          IS 250 Coolant Flush/Change   \n",
       "4                                            Gen1 mpg w/ dead battery?   \n",
       "...                                                                ...   \n",
       "19995                                                  Bilstein Shocks   \n",
       "19996                  Question on potential purchase of crashed bike.   \n",
       "19997                          Got our first warning light on our dash   \n",
       "19998                                          Any IS models to avoid?   \n",
       "19999  Advice please. Looking at a 2011 E550 with 71K miles. Good car?   \n",
       "\n",
       "                                                                                                  raw_text  \\\n",
       "0      Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...   \n",
       "1      I am trying to determine which is faster, and I've seen the dealership video with the two racing...   \n",
       "2      Hello! <lb><lb>Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... ...   \n",
       "3      https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/<lb><lb>I h...   \n",
       "4      Hi, new to this subreddit.  I'm considering buying a Gen1 Volt, but I can't find any straight an...   \n",
       "...                                                                                                    ...   \n",
       "19995  I read a lot Forums and people recommend getting TUNDRA Bilstein Shocks for a 3rd gen 4 runner, ...   \n",
       "19996  I am thinking about  buying a 2010 Harley Sportster 1200 custom for $6k with 7k miles. It has Va...   \n",
       "19997  My husband and I were headed somewhere and I was cold. So, I figured I could turn the car on bef...   \n",
       "19998  I am looking at getting a used Lexus IS (2014 model year and newer). Are there any trim levels t...   \n",
       "19999  Looking for some help. I've never owned any luxury car. Especially not a Mercedes. My husband's ...   \n",
       "\n",
       "      category      subcategory  \\\n",
       "0        autos  harley davidson   \n",
       "1        autos             ford   \n",
       "2        autos               VW   \n",
       "3        autos            lexus   \n",
       "4        autos        chevrolet   \n",
       "...        ...              ...   \n",
       "19995    autos           toyota   \n",
       "19996    autos  harley davidson   \n",
       "19997    autos        chevrolet   \n",
       "19998    autos            lexus   \n",
       "19999    autos         mercedes   \n",
       "\n",
       "                                                                                                      text  \n",
       "0      Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...  \n",
       "1      I am trying to determine which is faster, and I've seen the dealership video with the two racing...  \n",
       "2      Hello! Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... mine's g...  \n",
       "3      https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/ I have a I...  \n",
       "4      Hi, new to this subreddit. I'm considering buying a Gen1 Volt, but I can't find any straight ans...  \n",
       "...                                                                                                    ...  \n",
       "19995  I read a lot Forums and people recommend getting TUNDRA Bilstein Shocks for a 3rd gen 4 runner, ...  \n",
       "19996  I am thinking about buying a 2010 Harley Sportster 1200 custom for $6k with 7k miles. It has Van...  \n",
       "19997  My husband and I were headed somewhere and I was cold. So, I figured I could turn the car on bef...  \n",
       "19998  I am looking at getting a used Lexus IS (2014 model year and newer). Are there any trim levels t...  \n",
       "19999  Looking for some help. I've never owned any luxury car. Especially not a Mercedes. My husband's ...  \n",
       "\n",
       "[20000 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16d12f-e615-4819-8689-f0f0218a03f5",
   "metadata": {},
   "source": [
    "# 4.5 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c6911-0c27-45f3-b3b4-18b30e92ae06",
   "metadata": {},
   "source": [
    "## 정규표현식 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3f4fc10-9dd4-49d0-b9bf-c3a80fdacdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-08-10 23:32: @pete/@louis - I don't have a well-designed \n",
      "solution for today's problem. The code of module AC68 should be -1. \n",
      "Have to think a bit... #goodnight ;-) 😩😬\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "2019-08-10 23:32: @pete/@louis - I don't have a well-designed \n",
    "solution for today's problem. The code of module AC68 should be -1. \n",
    "Have to think a bit... #goodnight ;-) 😩😬\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb36db1f-2dd0-4ca6-acae-7c9abe8f9563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019|08|10|23|32|pete|louis|don|have|well|designed|solution|for|today|problem|The|code|of|module|AC68|should|be|Have|to|think|bit|goodnight\n"
     ]
    }
   ],
   "source": [
    "tokens=re.findall(r'\\w\\w+', text)\n",
    "print(*tokens,sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3037cb81-faad-4c0e-894d-88e987470b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23:32|@pete|@louis|I|don't|have|a|well-designed|solution|for|today's|problem|The|code|of|module|AC68|should|be|-1|Have|to|think|a|bit|#goodnight|;-)|😩|😬\n"
     ]
    }
   ],
   "source": [
    "RE_TOKEN = re.compile(r\"\"\"\n",
    "               ( [#]?[@\\w'’\\.\\-\\:]*\\w     # words, hash tags and email adresses\n",
    "               | [:;<]\\-?[\\)\\(3]          # coarse pattern for basic text emojis\n",
    "               | [\\U0001F100-\\U0001FFFF]  # coarse code range for unicode emojis\n",
    "               )\n",
    "               \"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return RE_TOKEN.findall(text)\n",
    "\n",
    "tokens=tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe17f96-613d-4374-bcff-bf1766ca4856",
   "metadata": {},
   "source": [
    "## NLTL 토큰화\n",
    "- word_tokenize() 함수는 내부적으로 PunkSentenceTokenizer 와 TreebankWordTokenizer를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "331b4b45-a7a2-4a06-8ab8-f2e3bf0fac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23:32|:|@|pete/|@|louis|-|I|do|n't|have|a|well-designed|solution|for|today|'s|problem|.|The|code|of|module|AC68|should|be|-1|.|Have|to|think|a|bit|...|#|goodnight|;|-|)|😩😬\n"
     ]
    }
   ],
   "source": [
    "import nltk # natural language toolkit\n",
    "tokens=nltk.tokenize.word_tokenize(text) # sentence -> word tokens\n",
    "print(*tokens,sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16d69b3d-5857-4261-b2fe-4f9cb07429ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23:32|@pete|@louis|I|don't|have|a|well-designed|solution|for|today's|problem|The|code|of|module|AC68|should|be|-1|Have|to|think|a|bit|#goodnight|;-)|😩|😬\n"
     ]
    }
   ],
   "source": [
    "# Regex Tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(RE_TOKEN.pattern, flags=re.VERBOSE)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31b1b726-b8e7-4f08-b27e-69f40ef230be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-|10|23:32|:|@pete|/|@louis|-|I|don't|have|a|well-designed|solution|for|today's|problem|.|The|code|of|module|AC68|should|be|-|1|.|Have|to|think|a|bit|...|#goodnight|;-)|😩|😬\n"
     ]
    }
   ],
   "source": [
    "# Tweet Tokenizer\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db6fc3b6-3bfc-437d-bc0d-f99914055100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23|:|32|:|@pete/@louis|-|I|don|'|t|have|a|well-designed|solution|for|today|'|s|problem.|The|code|of|module|AC68|should|be|-1.|Have|to|think|a|bit|...|#goodnight|;|-|)|😩😬\n"
     ]
    }
   ],
   "source": [
    "# Toktok Tokenizer\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25d049-4ca4-49ff-8a9c-4f7bcb2c272d",
   "metadata": {},
   "source": [
    "# 4.6 언어 처리: spacy\n",
    "- 스페이시의 파이프라인: from 텍스트 toward 문서\n",
    "- 토큰화 작업 -> 품사 태거 -> 의존분석 파서 -> 개체명 인식기 -> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23727e3-876b-4f90-bc20-383557c6c281",
   "metadata": {},
   "source": [
    "## 파이프라인 인스턴스화\n",
    "- tagger, parser, entityrecognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2cde8-c1c2-4396-86c9-d5c5b7732f84",
   "metadata": {},
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d554e9-7a67-499a-ae98-81003530022d",
   "metadata": {},
   "source": [
    "spacy.__version__\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70fb4a9b-292d-4f1d-b71d-550257ac16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5bc86065-23aa-4c4f-8c98-f253bc5171be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x19b2a45e630>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x19b2a45f830>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x19b2a20d700>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x19b2a6257d0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x19b2a616390>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x19b2a20d540>)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7bc583d1-b46c-406c-b94d-ea74c82b4e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.tagger.Tagger object at 0x0000019B32F22C30>)]\n"
     ]
    }
   ],
   "source": [
    "## 시간 효율성을 위하여 필요한 부분만 enalbe해둔다.\n",
    "nlp=spacy.load(\"en_core_web_sm\", enable=[\"tagger\"])\n",
    "## 다시 켤때는 enable\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f6921-6f86-449f-9730-5498d6b53a2b",
   "metadata": {},
   "source": [
    "## 텍스트 처리: display_nlp() 함수\n",
    "- return: spacy.tokens.doc.Doc 객체\n",
    "- 스페이시 내장 파이프 라인을 통해 생성되는 속성 목록\n",
    "- 1) Tokenizer: Token.is_punct, Token.is_alpha, Token.like_email. Token.like_url\n",
    "  2) Part-of-speech tagger: Token.pos_\n",
    "  3) Dependency parser: Token.dep_, Token.head, Doc.sents, Doc.noun_chunks(명사덩어리)\n",
    "  4) Named-entity recognizer: Doc.ents, Token.ent_iob_, Token.ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a1ceb84-2721-4f9a-ada9-a4b2df115b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Language.make_doc of <spacy.lang.en.English object at 0x0000019B32E4CE90>>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.make_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fbf40d1b-57dc-4480-9253-493e69fed294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My best friend Ryan Peters likes fancy adventure games.\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "print(nlp(text))\n",
    "print(type(nlp(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26cda73a-1773-42d2-98f6-82f4daaa07aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My|best|friend|Ryan|Peters|likes|fancy|adventure|games|.|"
     ]
    }
   ],
   "source": [
    "doc=nlp(text)\n",
    "for token in doc:\n",
    "    print(token, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c27938e-5cfe-49fe-8ef5-d251e821e26c",
   "metadata": {},
   "source": [
    "display_nlp 함수\n",
    "\n",
    "- 네, 이 코드는 Spacy의 Doc 객체를 분석하고, 각 토큰의 속성을 데이터프레임으로 표시하는 함수입니다. 각 속성에 대해 자세히 설명드리겠습니다:\r\n",
    "\r\n",
    "- `token`: 이는 토큰의 인덱스를 나타냅니다. Doc 객체는 여러 토큰으로 구성되며, 각 토큰은 고유한 인덱스를 가집니다.\r\n",
    "- `text`: 이는 토큰의 텍스트를 나타냅니다. 이는 원본 문서에서 토큰의 실제 텍스트를 나타냅니다.\r\n",
    "- `lemma_`: 이는 토큰의 기본형을 나타냅니다. 이는 단어의 원형을 나타내며, 동사의 경우 원형, 명사의 경우 단수형 등을 나타냅니다.\r\n",
    "- `is_stop`: 이는 토큰이 불용어인지 여부를 나타냅니다. 불용어는 'the', 'is', 'in'과 같이 문맥상 큰 의미가 없는 단어를 의미합니다.\r\n",
    "- `is_alpha`: 이는 토큰이 알파벳 문자로만 구성되어 있는지 여부를 나타냅니다.\r\n",
    "- `pos_`: 이는 토큰의 품사를 나타냅니다. 이는 'NOUN'(명사), 'VERB'(동사), 'ADJ'(형용사) 등과 같은 품사 정보를 제공합니다.\r\n",
    "- `dep_`: 이는 토큰의 의존 구문 관계를 나타냅니다. 이는 'nsubj'(주어), 'dobj'(목적어) 등과 같은 구문 관계를 나타냅니다.\r\n",
    "- `ent_type_`: 이는 토큰이 속한 개체명의 유형을 나타냅니다. 예를 들어, 'ORG'(조직), 'GPE'(지리적인 장소) 등의 유형이 있습니다.\r\n",
    "- `ent_iob_`: 이는 토큰이 개체명의 일부인지, 그리고 그 개체명이 시작하는지(i.e., 'B' for Beginning), 계속되는지(i.e., 'I' for Inside), 아니면 개체명이 아닌지(i.e., 'O' for Outside)를 나타냅니다.\r\n",
    "\r\n",
    "이 함수는 이러한 속성들을 각 토큰에 대해 수집하고, 이를 데이터프레임으로 변환하여 반환합니다. 이 데이터프레임은 자연어 처리 작업을 위한 초기 분석 단계에서 유용하게 사용될 수 있습니다. 다른 질문이 있으시면 언제든지 물어보세요. 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d5b0a70d-3b88-4153-8baa-e722860252c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nlp(doc,include_punct=False): # spacy Doc 객체를 매개변수로 받는다.\n",
    "    \"\"\"스페이시 토큰들의 시각화를 위한 데이터프레임 생성하기\"\"\"\n",
    "    rows=[]\n",
    "    for i, t in enumerate(doc):\n",
    "        if not t.is_punct or include_punct:\n",
    "            row={\n",
    "                'token': i, 'text':t.text, 'lemma_': t.lemma_,\n",
    "                'is_stop': t.is_stop, 'is_alpha':t.is_alpha,\n",
    "                'pos_':t.pos_, 'dep_':t.dep_,\n",
    "                'ent_type':t.ent_type_, 'ent_iob':t.ent_iob_\n",
    "            }\n",
    "            rows.append(row)\n",
    "    df=pd.DataFrame(rows).set_index('token')\n",
    "    df.index.name=None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ef40b25-ba4f-4486-bf06-7ee66427f67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type</th>\n",
       "      <th>ent_iob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>poss</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>friend</td>\n",
       "      <td>friend</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ryan</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peters</td>\n",
       "      <td>Peters</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>appos</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>likes</td>\n",
       "      <td>like</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fancy</td>\n",
       "      <td>fancy</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adventure</td>\n",
       "      <td>adventure</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>compound</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>games</td>\n",
       "      <td>game</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text     lemma_  is_stop  is_alpha   pos_      dep_ ent_type ent_iob\n",
       "0         My         my     True      True   PRON      poss                O\n",
       "1       best       good    False      True    ADJ      amod                O\n",
       "2     friend     friend    False      True   NOUN     nsubj                O\n",
       "3       Ryan       Ryan    False      True  PROPN  compound   PERSON       B\n",
       "4     Peters     Peters    False      True  PROPN     appos   PERSON       I\n",
       "5      likes       like    False      True   VERB      ROOT                O\n",
       "6      fancy      fancy    False      True    ADJ      amod                O\n",
       "7  adventure  adventure    False      True   NOUN  compound                O\n",
       "8      games       game    False      True   NOUN      dobj                O"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c86ad-2b96-4379-b21e-cb847af801f6",
   "metadata": {},
   "source": [
    "## 사용자정의 토큰화: custom_tokenizer() 함수\n",
    "- spacy의 토큰화는 #, - , _ 이 있을 때 분할하는 경우도 있으므로, 원하는 방식으로 작동시키려면, 동작을 조정해햐함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0188f764-4de8-4c7b-a17e-3123897e6eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low|-|carb|#|food|#|eat|-|smart|.|_|url|_|;-)|😋|👍|"
     ]
    }
   ],
   "source": [
    "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) 😋👍\"\n",
    "nlp = spacy.load('en_core_web_sm') ###\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")\n",
    "## - 을 기준으로 단어르 분할해버린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cddd5607-5948-4116-95eb-d14a781c4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "# prefix: 접두사, infix: 중위, suffix: 접미사\n",
    "def custom_tokenizer(nlp):\n",
    "\n",
    "    # re.search와 일치하는 패턴을 제외하고, 기본 패턴을 사용한다.\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes\n",
    "               if pattern not in ['-','_','#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "               if pattern not in ['_']]\n",
    "    infixes = [pattern for pattern in nlp.Defaults.infixes\n",
    "              if not re.search(pattern, 'xx-xx')]\n",
    "    return Tokenizer(vocab=nlp.vocab,\n",
    "                    rules=nlp.Defaults.tokenizer_exceptions,\n",
    "                    prefix_search=compile_prefix_regex(prefixes).search,\n",
    "                    suffix_search=compile_suffix_regex(suffixes).search,\n",
    "                    infix_finditer=compile_infix_regex(infixes).finditer,\n",
    "                    token_match = nlp.Defaults.token_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6e5966b-6452-499f-ab99-b55c86bb53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['§', '%', '=', '—', '–', '\\\\+(?![0-9])', '…', '……', ',', ':', ';', '\\\\!', '\\\\?', '¿', '؟', '¡', '\\\\(', '\\\\)', '\\\\[', '\\\\]', '\\\\{', '\\\\}', '<', '>', '_', '#', '\\\\*', '&', '。', '？', '！', '，', '、', '；', '：', '～', '·', '।', '،', '۔', '؛', '٪', '\\\\.\\\\.+', '…', \"\\\\'\", '\"', '”', '“', '`', '‘', '´', '’', '‚', ',', '„', '»', '«', '「', '」', '『', '』', '（', '）', '〔', '〕', '【', '】', '《', '》', '〈', '〉', '〈', '〉', '', '⟦', '⟧', '\\\\$', '£', '€', '¥', '฿', 'US\\\\$', 'C\\\\$', 'A\\\\$', '₽', '﷼', '₴', '₠', '₡', '₢', '₣', '₤', '₥', '₦', '₧', '₨', '₩', '₪', '₫', '€', '₭', '₮', '₯', '₰', '₱', '₲', '₳', '₴', '₵', '₶', '₷', '₸', '₹', '₺', '₻', '₼', '₽', '₾', '₿', '[\\\\u00A6\\\\u00A9\\\\u00AE\\\\u00B0\\\\u0482\\\\u058D\\\\u058E\\\\u060E\\\\u060F\\\\u06DE\\\\u06E9\\\\u06FD\\\\u06FE\\\\u07F6\\\\u09FA\\\\u0B70\\\\u0BF3-\\\\u0BF8\\\\u0BFA\\\\u0C7F\\\\u0D4F\\\\u0D79\\\\u0F01-\\\\u0F03\\\\u0F13\\\\u0F15-\\\\u0F17\\\\u0F1A-\\\\u0F1F\\\\u0F34\\\\u0F36\\\\u0F38\\\\u0FBE-\\\\u0FC5\\\\u0FC7-\\\\u0FCC\\\\u0FCE\\\\u0FCF\\\\u0FD5-\\\\u0FD8\\\\u109E\\\\u109F\\\\u1390-\\\\u1399\\\\u1940\\\\u19DE-\\\\u19FF\\\\u1B61-\\\\u1B6A\\\\u1B74-\\\\u1B7C\\\\u2100\\\\u2101\\\\u2103-\\\\u2106\\\\u2108\\\\u2109\\\\u2114\\\\u2116\\\\u2117\\\\u211E-\\\\u2123\\\\u2125\\\\u2127\\\\u2129\\\\u212E\\\\u213A\\\\u213B\\\\u214A\\\\u214C\\\\u214D\\\\u214F\\\\u218A\\\\u218B\\\\u2195-\\\\u2199\\\\u219C-\\\\u219F\\\\u21A1\\\\u21A2\\\\u21A4\\\\u21A5\\\\u21A7-\\\\u21AD\\\\u21AF-\\\\u21CD\\\\u21D0\\\\u21D1\\\\u21D3\\\\u21D5-\\\\u21F3\\\\u2300-\\\\u2307\\\\u230C-\\\\u231F\\\\u2322-\\\\u2328\\\\u232B-\\\\u237B\\\\u237D-\\\\u239A\\\\u23B4-\\\\u23DB\\\\u23E2-\\\\u2426\\\\u2440-\\\\u244A\\\\u249C-\\\\u24E9\\\\u2500-\\\\u25B6\\\\u25B8-\\\\u25C0\\\\u25C2-\\\\u25F7\\\\u2600-\\\\u266E\\\\u2670-\\\\u2767\\\\u2794-\\\\u27BF\\\\u2800-\\\\u28FF\\\\u2B00-\\\\u2B2F\\\\u2B45\\\\u2B46\\\\u2B4D-\\\\u2B73\\\\u2B76-\\\\u2B95\\\\u2B98-\\\\u2BC8\\\\u2BCA-\\\\u2BFE\\\\u2CE5-\\\\u2CEA\\\\u2E80-\\\\u2E99\\\\u2E9B-\\\\u2EF3\\\\u2F00-\\\\u2FD5\\\\u2FF0-\\\\u2FFB\\\\u3004\\\\u3012\\\\u3013\\\\u3020\\\\u3036\\\\u3037\\\\u303E\\\\u303F\\\\u3190\\\\u3191\\\\u3196-\\\\u319F\\\\u31C0-\\\\u31E3\\\\u3200-\\\\u321E\\\\u322A-\\\\u3247\\\\u3250\\\\u3260-\\\\u327F\\\\u328A-\\\\u32B0\\\\u32C0-\\\\u32FE\\\\u3300-\\\\u33FF\\\\u4DC0-\\\\u4DFF\\\\uA490-\\\\uA4C6\\\\uA828-\\\\uA82B\\\\uA836\\\\uA837\\\\uA839\\\\uAA77-\\\\uAA79\\\\uFDFD\\\\uFFE4\\\\uFFE8\\\\uFFED\\\\uFFEE\\\\uFFFC\\\\uFFFD\\\\U00010137-\\\\U0001013F\\\\U00010179-\\\\U00010189\\\\U0001018C-\\\\U0001018E\\\\U00010190-\\\\U0001019B\\\\U000101A0\\\\U000101D0-\\\\U000101FC\\\\U00010877\\\\U00010878\\\\U00010AC8\\\\U0001173F\\\\U00016B3C-\\\\U00016B3F\\\\U00016B45\\\\U0001BC9C\\\\U0001D000-\\\\U0001D0F5\\\\U0001D100-\\\\U0001D126\\\\U0001D129-\\\\U0001D164\\\\U0001D16A-\\\\U0001D16C\\\\U0001D183\\\\U0001D184\\\\U0001D18C-\\\\U0001D1A9\\\\U0001D1AE-\\\\U0001D1E8\\\\U0001D200-\\\\U0001D241\\\\U0001D245\\\\U0001D300-\\\\U0001D356\\\\U0001D800-\\\\U0001D9FF\\\\U0001DA37-\\\\U0001DA3A\\\\U0001DA6D-\\\\U0001DA74\\\\U0001DA76-\\\\U0001DA83\\\\U0001DA85\\\\U0001DA86\\\\U0001ECAC\\\\U0001F000-\\\\U0001F02B\\\\U0001F030-\\\\U0001F093\\\\U0001F0A0-\\\\U0001F0AE\\\\U0001F0B1-\\\\U0001F0BF\\\\U0001F0C1-\\\\U0001F0CF\\\\U0001F0D1-\\\\U0001F0F5\\\\U0001F110-\\\\U0001F16B\\\\U0001F170-\\\\U0001F1AC\\\\U0001F1E6-\\\\U0001F202\\\\U0001F210-\\\\U0001F23B\\\\U0001F240-\\\\U0001F248\\\\U0001F250\\\\U0001F251\\\\U0001F260-\\\\U0001F265\\\\U0001F300-\\\\U0001F3FA\\\\U0001F400-\\\\U0001F6D4\\\\U0001F6E0-\\\\U0001F6EC\\\\U0001F6F0-\\\\U0001F6F9\\\\U0001F700-\\\\U0001F773\\\\U0001F780-\\\\U0001F7D8\\\\U0001F800-\\\\U0001F80B\\\\U0001F810-\\\\U0001F847\\\\U0001F850-\\\\U0001F859\\\\U0001F860-\\\\U0001F887\\\\U0001F890-\\\\U0001F8AD\\\\U0001F900-\\\\U0001F90B\\\\U0001F910-\\\\U0001F93E\\\\U0001F940-\\\\U0001F970\\\\U0001F973-\\\\U0001F976\\\\U0001F97A\\\\U0001F97C-\\\\U0001F9A2\\\\U0001F9B0-\\\\U0001F9B9\\\\U0001F9C0-\\\\U0001F9C2\\\\U0001F9D0-\\\\U0001F9FF\\\\U0001FA60-\\\\U0001FA6D]']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3d720f0a-4960-4bc4-bd22-eee5bfbb551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|😋|👍|"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer=custom_tokenizer(nlp)\n",
    "\n",
    "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) 😋👍\"\n",
    "doc=nlp(text)\n",
    "\n",
    "print(nlp.tokenizer.find_prefix('#'))\n",
    "print(nlp.tokenizer.find_prefix('%'))\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end='|')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f635cb69-b9ab-4eb3-a1f1-6efc04bb4e00",
   "metadata": {},
   "source": [
    "## 불용어 제거: nlp.vocab[].is_stop="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d196adfe-8a1f-491e-a928-3e49a765ffce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dear, Ryan, need, sit, talk, Regards, Pete]\n"
     ]
    }
   ],
   "source": [
    "text=\"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc=nlp(text) \n",
    "non_stop=[t for t in doc if not t.is_stop and not t.is_punct]\n",
    "print(non_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9bd76aba-3c3e-4902-939e-6f0e422aff07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lexeme.Lexeme at 0x19b5543bf00>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "nlp.vocab['down'].is_stop=False # 'down'을 스탑워드에서 제거하기\n",
    "nlp.vocab['Dear'].is_stop=True # 'Dear'를 스탑워드에 추가하기\n",
    "## nlp.vocab은 아마도 딕셔너리 형태일 것으로 추정\n",
    "nlp.vocab['Regards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bb596953-77b8-4dfe-9d0f-e499290b93a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ryan, need, sit, down, talk, Regards, Pete]\n"
     ]
    }
   ],
   "source": [
    "text=\"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc=nlp(text) \n",
    "non_stop=[t for t in doc if not t.is_stop and not t.is_punct]\n",
    "print(non_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae84dc3-0941-44b0-b1c1-a67e28223935",
   "metadata": {},
   "source": [
    "## 품사기반 단어원형 추출: token.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f8ad0e23-0c81-4185-9616-0068eccae74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my|good|friend|Ryan|Peters|like|fancy|adventure|game|.\n"
     ]
    }
   ],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(*[t.lemma_ for t in doc], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0952f473-4863-48b5-b08e-e1f3d86bce36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[friend, Ryan, Peters, adventure, games]\n"
     ]
    }
   ],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "nouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c2bd7d6c-df78-4a13-aeff-a97fea11465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best|friend|fancy|adventure|games\n"
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "\n",
    "tokens = textacy.extract.words(doc, \n",
    "            filter_stops = True,           # default True, 불용어를 허용하지 않는다.\n",
    "            filter_punct = True,           # default True, 구두점을 허용하지 않는다.\n",
    "            filter_nums = True,            # default False, 숫자를 허용하지 않는다.\n",
    "            include_pos = ['ADJ', 'NOUN'], # default None = 모든 품사를 허용한다.(여기서는 형용사와 명사만 허용)\n",
    "            exclude_pos = None,            # default None = 모든 품사를 배제하지 않는다. (여기서는 형용사와 명사 제외한 모든 품사 제외)\n",
    "            min_freq = 1)                  # minimum frequency of words\n",
    "\n",
    "print(*[t for t in tokens], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4dadcbd4-ca80-4404-ab24-921ff82f8371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best|friend|fancy|adventure|games\n"
     ]
    }
   ],
   "source": [
    "lemmas = textacy.extract.words(doc, include_pos=['ADJ','NOUN'])\n",
    "print(*lemmas, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f0612-57fa-4a6b-8db5-0cd0c96137f1",
   "metadata": {},
   "source": [
    "## 명사구 추출: doc.noun_chunks 및 extract_noun_phrase() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "224712b5-044e-4d83-9a01-701f565beb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good friend|fancy adventure|fancy adventure game\n",
      "My best friend|Ryan Peters|fancy adventure games\n"
     ]
    }
   ],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc=nlp(text)\n",
    "\n",
    "patterns=[\"POS:ADJ POS:NOUN:+\"] # 형용사 명사 조합만을 추출하기 위한 전략\n",
    "spans=textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "print(*[s.lemma_ for s in spans], sep='|')\n",
    "\n",
    "print(*doc.noun_chunks, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b5e5cf22-3675-4d54-9da1-3f7cd7956316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good friend|fancy adventure|fancy adventure game|adventure game\n"
     ]
    }
   ],
   "source": [
    "def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep=' '):\n",
    "    patterns=[]\n",
    "    for pos in preceding_pos:\n",
    "        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n",
    "\n",
    "    spans=textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "    return [sep.join([t.lemma_ for t in s]) for s in spans] ## lemmatization 까지 한 상태로 match된 토큰들을 반환\n",
    "print(*extract_noun_phrases(doc,['ADJ', 'NOUN']), sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a843a-711b-474a-bc98-c86ed79c19e0",
   "metadata": {},
   "source": [
    "## 개체명 추출: displacy 모듈 및 extract_entities() 함수\n",
    "- ent_type_: entity type (person 등)\n",
    "- ent_iob_:\n",
    "- - B: 해당 토큰에서 개체가 시작되는 것인지.\n",
    "  - I: 개체의 일부분인지.\n",
    "  - O: 개체가 아닌지."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1ffa2def-ef44-41ee-a3b1-b2de23f41b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(James O'Neill, PERSON) (World Cargo Inc, ORG) (San Francisco, GPE) "
     ]
    }
   ],
   "source": [
    "text = \"James O'Neill, chairman of World Cargo Inc, lives in San Francisco.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text}, {ent.label_})\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c537a384-5bfc-490a-a32d-cc08e4f9a112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    James O'Neill\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", chairman of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World Cargo Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", lives in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Francisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "396701e8-e754-4a51-9258-8d9b8fd59b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"James O'Neill/PERSON\", 'San Francisco/GPE']\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(doc, include_types=None, sep=' '):\n",
    "    ents= textacy.extract.entities(doc,\n",
    "                                  include_types=include_types,\n",
    "                                  exclude_types=None,\n",
    "                                  drop_determiners=True,\n",
    "                                  min_freq=1)\n",
    "    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]\n",
    "\n",
    "print(extract_entities(doc, ['PERSON','GPE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad4c2c-2eed-4d5c-a6de-387847ea165a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
